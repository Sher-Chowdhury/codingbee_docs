# emptyDir Volumes

When a container is terminated, then the data inside the container (if any) also gets deleted. For example if a mysql database is running inside a container, then the chances are that you don't want to lose that data when the mysql container dies for any reason. That's where Kubernetes volumes comes into the picture. Volumes let's you store your data outside of your containers, your containers can then mount these volumes in the same way you mount block devices on a Linux machine. There are a lot of different types of Kubernetes volumes, one of the most basic type is the [emptyDir volume](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir). 

emptyDir volumes exists outside a container but inside the pod. This setup has the following characteristics:

- If the container dies, then kubernetes will launch a new container inside the pod, and that container will reconnect to the volume and pick up where the previous container left off
- emptyDir volumes are only mountable by containers in the same pod.
- The underlying storage used by emptyDir volumes is the worker node's disk space. 

One possible use case for using emptyDir is when you have a 2 container pod, where the main container app outputs logs, and the sidecar container is a log-forwarder that will forward your log to a log aggregator such as ELK server. In that scenario, you can have a emptyDir volume mounted on to the directory where your main app writes logs to. Then on the sidecar container, you mount the same volume so that it can read the logs and forward them to the elk server.

However we don't have an ELK server to demonstrate this, so in our demo we'll create a 2-container pod, the main container is a httpd container, and the sidecar container is a centos container. The centos container will create the index.html file which the httpd container will display via an emptyDir volume:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo
  labels:
    component: httpd_pods
spec: 
  volumes: 
    - name: webcontent    # here we specify the name of the volume
      emptyDir: {}        # the '{}' is yaml syntax to represent an empty dictionary
  containers:
    - name: cntr-httpd
      image: httpd
      volumeMounts:                        # Here we mount the volume
        - name: webcontent
          mountPath: /usr/local/apache2/htdocs
      ports:
        - containerPort: 80
    - name: cntr-centos
      image: centos
      volumeMounts:                        # Here we mount the volume
        - name: webcontent
          mountPath: /tmp/reports
      command: ["/bin/bash", "-c"]
      args:
        - |
          while true ; do
            echo "This content is being generated by the centos container, $(hostname)" >> /tmp/reports/index.html
            date >> /tmp/reports/index.html
            sleep 20
          done
```

After apply the above, let's confirm the 2 pods exist:

```bash
$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
pod-demo   2/2     Running   0          17s   172.17.0.7   minikube   <none>           <none>
```

This shows 2/2 containers are running. Now if we curl the service:

```bash
$ curl $(minikube service svc-nodeport-httpd --url)
This content is being generated by the centos container, pod-demo
Thu Mar 14 23:03:04 UTC 2019
This content is being generated by the centos container, pod-demo
Thu Mar 14 23:03:24 UTC 2019
This content is being generated by the centos container, pod-demo
Thu Mar 14 23:03:44 UTC 2019
```

This shows that the file that's being updated by the centos container is being display by the httpd container. Now let's try killing both containers, we can't do this via kubectl, but we can using docker cli:

```bash
$ eval $(minikube docker-env)
$ docker ps | grep pod-demo
1465e96a6f81        centos                                                           "/bin/bash -c 'while…"   About a minute ago   Up About a minute                                                                        k8s_cntr-centos_pod-demo_default_4fa2a545-46ad-11e9-b5c9-08002722e1d0_0
7e16e06dd0a9        httpd                                                            "httpd-foreground"       About a minute ago   Up About a minute                                                                        k8s_cntr-httpd_pod-demo_default_4fa2a545-46ad-11e9-b5c9-08002722e1d0_0
34d15be5fe08        k8s.gcr.io/pause:3.1                                             "/pause"                 About a minute ago   Up About a minute                                                                        k8s_POD_pod-demo_default_4fa2a545-46ad-11e9-b5c9-08002722e1d0_0


$ docker kill 1465e96a6f81 7e16e06dd0a9 
1465e96a6f81
7e16e06dd0a9


$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES
pod-demo   2/2     Running   2          2m25s   172.17.0.7   minikube   <none>           <none>
```

We can now see the 2 restarts, indicating two containers have been restarted. 



```bash
$ kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
pod-demo   2/2     Running   1          13m   172.17.0.7   minikube   <none>           <none>
```

However if we run the curl test again:

```bash
$ curl $(minikube service svc-nodeport-httpd --url)
This content is being generated by the centos container, pod-demo
Thu Mar 14 23:03:04 UTC 2019
This content is being generated by the centos container, pod-demo
Thu Mar 14 23:03:24 UTC 2019
This content is being generated by the centos container, pod-demo
Thu Mar 14 23:03:44 UTC 2019
This content is being generated by the centos container, pod-demo
Thu Mar 14 23:04:04 UTC 2019
This content is being generated by the centos container, pod-demo
```

This output still shows the original timestamps in the beginning proving that even after killing all containers in the pod, the data still persisted and became available again after the pod's containers restarted. 

However since emptyDir volume lives inside the pod, it means that the emptyDir volume and it's data will still get deleted if the pod itself is terminated. Therefore you should only use emptyDir volumes for storing data that's only of value during the pod's lifetime. So if you want your data to persist after a pod's termination, then you need to store the data outside the pod, one way to do this is using hostPath volumes, which we'll cover next.