# Networking Basics

In Kubernetes, 'services' is actually all about networking. In Docker world, when you use docker-compose, all the networking is done for you automatically behind the scenes. With Kubernetes on the other hand, you will need to set up a lot of the networking. However there are some basic networking features that comes out-of-the-box with Kubernetes:

- A pod's internal networking
- IP based pod-to-pod networking
- dns-service (kube-dns)


  
  
## A pod's internal networking

If you have 2+ containers inside a single pod, then these containers can reach each other via localhost. Let's say we create the following pod:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-centos
  labels:
    component: centos
spec:
  containers:
    - name: cntr-httpd
      image: httpd:latest
      ports:
        - containerPort: 80
    - name: cntr-centos
      image: centos
      command: ["/bin/bash", "-c"]
      args:
        - |
          while true ; do
            date 
            curl http://localhost
            sleep 10 
          done
```

This defines a single pod that's housing 2 containers (and in our case kubernetes has auto-assigned an IP of 172.17.0.7):

```bash
$ kubectl get pods -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
pod-centos   2/2     Running   0          74m   172.17.0.7   minikube   <none>           <none>
```

These containers should be able to talk to each other without needing any further configurations. You'll find is that each container is attached to 2 network interfaces:

```bash
[root@pod-centos /]# yum install -q -y net-tools
warning: /var/cache/yum/x86_64/7/base/packages/net-tools-2.0-0.24.20131004git.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
Public key for net-tools-2.0-0.24.20131004git.el7.x86_64.rpm is not installed
Importing GPG key 0xF4A80EB5:
 Userid     : "CentOS-7 Key (CentOS 7 Official Signing Key) <security@centos.org>"
 Fingerprint: 6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5
 Package    : centos-release-7-6.1810.2.el7.centos.x86_64 (@CentOS)
 From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
[root@pod-centos /]# ifconfig
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.0.7  netmask 255.255.0.0  broadcast 172.17.255.255
        ether 02:42:ac:11:00:07  txqueuelen 0  (Ethernet)
        RX packets 3475  bytes 9754927 (9.3 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 2819  bytes 161157 (157.3 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 40  bytes 3516 (3.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 40  bytes 3516 (3.4 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```

These are virtual network interfaces that exists as part of the pod. These same network interfaces are actually attached to all the containers in the pod at the same time. Because of that, it means that, from cntr-centos container you can reach the cntr-httpd via either of these interfaces ip address:

```bash
[root@pod-centos /]# curl http://127.0.0.1
<html><body><h1>It works!</h1></body></html>
[root@pod-centos /]# curl http://172.17.0.7
<html><body><h1>It works!</h1></body></html>
```

Here our pod routes this curl request internally to the cntr-httpd container.

**Question:** If our pod had several other containers, then how would our pod know which container to route the curl request to? 
**Answer:** Our yaml file specifies which port each container is listening on (if any). Therefore our pod knows that requests destined to port 80 should be routed to cntr-httpd container.



You also find that the following works as well:

```bash
[root@pod-centos /]# curl http://localhost
<html><body><h1>It works!</h1></body></html>
```

That's because all the containers in the pods has the following hosts file, which resolves 'localhost' to it's internal network interface, 127.0.0.1:

```bash
[root@pod-centos /]# cat /etc/hosts
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
172.17.0.7      pod-centos
```

Furthermore all containers in a pod, have the same hostname, which by default is the pod's name:

```bash
[root@pod-centos /]# hostname
pod-centos
```

and since we have an entry in the hosts file for this hostname, it means the following also works:

```bash
[root@pod-centos /]# curl http://pod-centos
<html><body><h1>It works!</h1></body></html>
```

In both 'locahost' and 'hostname' curl requests, a static internal dns-lookup occured thanks to the /etc/hosts entry.

So far we did a **cntr-centos**-->**cntr-httpd**. But we didn't do it in the other direction. That's because cntr-centos is in theory reachable, but we haven't setup anytihng on cntr-centos to listen on any ports.



## IP based Pod-2-Pod networking

[Pod-2-pod](https://kubernetes.io/docs/concepts/cluster-administration/networking/) is possible thanks to the fact that Kubernetes auto-assigns an private ip address to all pods during the pod's launch time. The pods can reach each other with these ip addresses.

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-httpd
spec:
  containers:
    - name: cntr-httpd
      image: httpd:latest
      ports:
        - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
    - name: cntr-httpd
      image: nginx:latest
      ports:
        - containerPort: 80
```

The pod's ip are shown below.

```bash
$ kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
pod-httpd   1/1     Running   0          54s   172.17.0.8   minikube   <none>           <none>
pod-nginx   1/1     Running   0          54s   172.17.0.7   minikube   <none>           <none>
```

To test this, we need to log into one of the pods, and run curl to the other pod. Let's log into pod-httpd:

```bash
kubectl exec pod-httpd  -it /bin/bash
root@pod-httpd:/usr/local/apache2# apt-get update
root@pod-httpd:/usr/local/apache2# apt-get install -y curl
```

We also had to install the curl command itself. Before we can perform the test:


```bash
root@pod-httpd:/usr/local/apache2# curl http://172.17.0.7
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

However we can't rely on IP addresses because they are prone to changing, e.g. when a pod is rebuilt. So hard coding ip addresses in various places is not an option. The conventional to address this problem is giving each pod a human-readable dns name, which is kept up to date dynamically, that's where kube-dns comes to the rescue


#Â DNS Service

Minikube comes with a builtin internal dns service, kube-dns ([soon to be coredn](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/)).


```bash
$ kubectl get svc kube-dns --namespace=kube-system -o wide
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE    SELECTOR
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP   3d3h   k8s-app=kube-dns
```

This ip address is automatically added to all other pods /etc/resolv.conf file:

```bash
root@pod-nginx:/# cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

So all we need to do is create dns entries in our internal dns server (kube-dns). That's done by creating service objects. There are different types of service, such as nodePort, and ClusterIP service. We'll cover them later. 